{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6785341",
   "metadata": {},
   "source": [
    "# Avatar: Ash & Fire - First Week Box Office Prediction\n",
    "\n",
    "## Machine Learning Project with Stacking & Ensemble Models\n",
    "\n",
    "This notebook builds a comprehensive ML pipeline to predict the first week box office income of **Avatar: Ash & Fire** using:\n",
    "- **IMDB Data**: Ratings, votes, metascores\n",
    "- **Box Office Mojo**: Historical performance, theater counts\n",
    "- **Trend Data (T-7)**: Social media engagement 7 days before release\n",
    "\n",
    "### Methodology:\n",
    "1. Data Collection & Feature Engineering\n",
    "2. Multiple Base Models (10+ algorithms)\n",
    "3. Stacking Ensemble\n",
    "4. Voting Ensemble\n",
    "5. Model Evaluation & Selection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f5582",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, modeling, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3aa4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Base Models\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n",
    "                              ExtraTreesRegressor, AdaBoostRegressor,\n",
    "                              StackingRegressor, VotingRegressor)\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Advanced Models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0502b",
   "metadata": {},
   "source": [
    "## 2. Data Collection and Loading\n",
    "\n",
    "Create historical dataset with features from IMDB, Box Office Mojo, and T-7 Trend Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive historical dataset\n",
    "# This simulates data collection from IMDB, Box Office Mojo, and Trend sources\n",
    "\n",
    "def create_historical_dataset(n_samples=100):\n",
    "    \"\"\"\n",
    "    Create historical movie dataset with features from multiple sources\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Base movie data\n",
    "    movies = []\n",
    "    \n",
    "    # Example blockbuster movies for training\n",
    "    base_movies = [\n",
    "        {'title': 'Avatar', 'budget': 237000000, 'first_week': 150000000, 'genre': 'Action|Sci-Fi', 'director': 'James Cameron'},\n",
    "        {'title': 'Avatar: The Way of Water', 'budget': 350000000, 'first_week': 250000000, 'genre': 'Action|Sci-Fi', 'director': 'James Cameron'},\n",
    "        {'title': 'Avengers: Endgame', 'budget': 356000000, 'first_week': 600000000, 'genre': 'Action|Adventure', 'director': 'Russo Brothers'},\n",
    "        {'title': 'Star Wars: The Force Awakens', 'budget': 245000000, 'first_week': 390000000, 'genre': 'Action|Sci-Fi', 'director': 'J.J. Abrams'},\n",
    "        {'title': 'Jurassic World', 'budget': 150000000, 'first_week': 300000000, 'genre': 'Action|Adventure', 'director': 'Colin Trevorrow'},\n",
    "        {'title': 'The Lion King', 'budget': 260000000, 'first_week': 280000000, 'genre': 'Animation|Adventure', 'director': 'Jon Favreau'},\n",
    "        {'title': 'Black Panther', 'budget': 200000000, 'first_week': 320000000, 'genre': 'Action|Adventure', 'director': 'Ryan Coogler'},\n",
    "        {'title': 'Spider-Man: No Way Home', 'budget': 200000000, 'first_week': 400000000, 'genre': 'Action|Adventure', 'director': 'Jon Watts'},\n",
    "    ]\n",
    "    \n",
    "    # Generate synthetic data based on base movies\n",
    "    for i in range(n_samples):\n",
    "        base = base_movies[i % len(base_movies)]\n",
    "        \n",
    "        # Add variation to create training samples\n",
    "        noise_factor = np.random.uniform(0.7, 1.3)\n",
    "        \n",
    "        movie = {\n",
    "            # Basic Info\n",
    "            'movie_title': f\"{base['title']}_sample_{i}\",\n",
    "            'budget': base['budget'] * noise_factor,\n",
    "            'genre': base['genre'],\n",
    "            'director': base['director'],\n",
    "            'runtime': np.random.randint(120, 200),\n",
    "            \n",
    "            # Target Variable\n",
    "            'first_week': base['first_week'] * noise_factor,\n",
    "            \n",
    "            # IMDB Features\n",
    "            'imdb_rating': np.random.uniform(6.5, 9.0),\n",
    "            'imdb_votes': np.random.randint(50000, 800000),\n",
    "            'metascore': np.random.randint(60, 95),\n",
    "            'num_reviews': np.random.randint(300, 2500),\n",
    "            'num_critic_reviews': np.random.randint(30, 250),\n",
    "            \n",
    "            # Box Office Features\n",
    "            'num_theaters': np.random.randint(3000, 4700),\n",
    "            'opening_weekend': base['first_week'] * noise_factor * np.random.uniform(0.4, 0.6),\n",
    "            'average_per_theater': np.random.randint(15000, 85000),\n",
    "            \n",
    "            # T-7 Trend Data (7 days before release)\n",
    "            'twitter_mentions': np.random.randint(30000, 250000),\n",
    "            'twitter_sentiment': np.random.uniform(0.5, 0.95),\n",
    "            'google_trends_score': np.random.uniform(60, 100),\n",
    "            'youtube_trailer_views': np.random.randint(5000000, 60000000),\n",
    "            'youtube_trailer_likes': np.random.randint(200000, 2500000),\n",
    "            'instagram_hashtag_count': np.random.randint(50000, 600000),\n",
    "            'facebook_page_likes': np.random.randint(500000, 6000000),\n",
    "            'reddit_mentions': np.random.randint(500, 15000),\n",
    "            'search_volume_index': np.random.uniform(50, 100),\n",
    "            'ticket_presales': np.random.randint(3000000, 25000000),\n",
    "            \n",
    "            # Temporal Features\n",
    "            'release_month': np.random.randint(1, 13),\n",
    "            'release_day_of_week': np.random.randint(0, 7),\n",
    "            'is_holiday_season': np.random.choice([0, 1], p=[0.7, 0.3]),\n",
    "            'is_summer': np.random.choice([0, 1], p=[0.6, 0.4]),\n",
    "            'competing_releases_same_week': np.random.randint(0, 4),\n",
    "            \n",
    "            # Franchise Features\n",
    "            'is_sequel': np.random.choice([0, 1], p=[0.4, 0.6]),\n",
    "            'franchise_previous_avg_gross': np.random.uniform(200000000, 800000000),\n",
    "            'years_since_last_release': np.random.randint(1, 6),\n",
    "        }\n",
    "        \n",
    "        movies.append(movie)\n",
    "    \n",
    "    df = pd.DataFrame(movies)\n",
    "    \n",
    "    # Add some correlation between features and target\n",
    "    df['first_week'] = (\n",
    "        df['budget'] * 0.4 + \n",
    "        df['imdb_rating'] * 15000000 +\n",
    "        df['twitter_mentions'] * 2 +\n",
    "        df['youtube_trailer_views'] * 0.008 +\n",
    "        df['ticket_presales'] * 10 +\n",
    "        df['num_theaters'] * 50000 +\n",
    "        np.random.normal(0, 20000000, len(df))\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create dataset\n",
    "df = create_historical_dataset(n_samples=100)\n",
    "\n",
    "print(f\"Dataset created with {len(df)} samples and {len(df.columns)} features\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79994e73",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a61ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(f\"\\n\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(f\"\\n\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\n\\nTarget Variable Distribution:\")\n",
    "print(f\"Mean First Week Income: ${df['first_week'].mean():,.2f}\")\n",
    "print(f\"Median First Week Income: ${df['first_week'].median():,.2f}\")\n",
    "print(f\"Min First Week Income: ${df['first_week'].min():,.2f}\")\n",
    "print(f\"Max First Week Income: ${df['first_week'].max():,.2f}\")\n",
    "print(f\"Std Dev: ${df['first_week'].std():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9787321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['first_week'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('First Week Income ($)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of First Week Box Office Income', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df['first_week'], vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue'))\n",
    "axes[1].set_ylabel('First Week Income ($)', fontsize=12)\n",
    "axes[1].set_title('Box Plot of First Week Income', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89441299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for key features\n",
    "key_features = ['first_week', 'budget', 'imdb_rating', 'imdb_votes', 'num_theaters', \n",
    "                'twitter_mentions', 'youtube_trailer_views', 'ticket_presales', \n",
    "                'google_trends_score', 'twitter_sentiment']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[key_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Correlation Matrix of Key Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop correlations with first_week income:\")\n",
    "correlations = correlation_matrix['first_week'].sort_values(ascending=False)\n",
    "print(correlations[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb17c5",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Create advanced features through interactions, polynomial transformations, and ratio calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced features from raw data\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"Creating engineered features...\")\n",
    "    \n",
    "    # === Interaction Features ===\n",
    "    df['budget_rating_interaction'] = df['budget'] * df['imdb_rating']\n",
    "    df['budget_per_theater'] = df['budget'] / (df['num_theaters'] + 1)\n",
    "    df['views_per_like'] = df['youtube_trailer_views'] / (df['youtube_trailer_likes'] + 1)\n",
    "    \n",
    "    # === Social Media Engagement Score ===\n",
    "    df['social_engagement_score'] = (\n",
    "        df['twitter_mentions'] * 0.3 +\n",
    "        df['youtube_trailer_views'] * 0.5 / 10000 +\n",
    "        df['instagram_hashtag_count'] * 0.2\n",
    "    )\n",
    "    \n",
    "    # === Hype Index (T-7 data combined) ===\n",
    "    df['hype_index'] = (\n",
    "        df['google_trends_score'] * 0.4 +\n",
    "        df['twitter_sentiment'] * 100 * 0.3 +\n",
    "        (df['ticket_presales'] / df['ticket_presales'].max() * 100) * 0.3\n",
    "    )\n",
    "    \n",
    "    # === Quality Score ===\n",
    "    df['quality_score'] = (\n",
    "        df['imdb_rating'] * 10 * 0.6 +\n",
    "        df['metascore'] * 0.4\n",
    "    )\n",
    "    \n",
    "    # === Theater Efficiency ===\n",
    "    df['theater_efficiency'] = df['average_per_theater'] / (df['num_theaters'] + 1)\n",
    "    \n",
    "    # === Franchise Momentum ===\n",
    "    df['franchise_momentum'] = df['franchise_previous_avg_gross'] / (df['years_since_last_release'] + 1)\n",
    "    \n",
    "    # === Polynomial Features ===\n",
    "    df['budget_squared'] = df['budget'] ** 2\n",
    "    df['budget_log'] = np.log1p(df['budget'])\n",
    "    df['imdb_rating_squared'] = df['imdb_rating'] ** 2\n",
    "    df['num_theaters_squared'] = df['num_theaters'] ** 2\n",
    "    \n",
    "    # === Ratio Features ===\n",
    "    df['budget_per_vote'] = df['budget'] / (df['imdb_votes'] + 1)\n",
    "    df['presales_per_theater'] = df['ticket_presales'] / (df['num_theaters'] + 1)\n",
    "    df['youtube_engagement_rate'] = df['youtube_trailer_likes'] / (df['youtube_trailer_views'] + 1)\n",
    "    \n",
    "    # === Temporal Features (Cyclical Encoding) ===\n",
    "    df['release_month_sin'] = np.sin(2 * np.pi * df['release_month'] / 12)\n",
    "    df['release_month_cos'] = np.cos(2 * np.pi * df['release_month'] / 12)\n",
    "    df['release_day_sin'] = np.sin(2 * np.pi * df['release_day_of_week'] / 7)\n",
    "    df['release_day_cos'] = np.cos(2 * np.pi * df['release_day_of_week'] / 7)\n",
    "    \n",
    "    # === Genre Encoding ===\n",
    "    genres = ['Action', 'Sci-Fi', 'Adventure', 'Animation', 'Drama']\n",
    "    for genre in genres:\n",
    "        df[f'genre_{genre.lower()}'] = df['genre'].str.contains(genre, case=False, na=False).astype(int)\n",
    "    \n",
    "    print(f\"âœ“ Feature engineering complete! New shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df_engineered = engineer_features(df)\n",
    "\n",
    "print(f\"\\nOriginal features: {df.shape[1]}\")\n",
    "print(f\"Engineered features: {df_engineered.shape[1]}\")\n",
    "print(f\"New features added: {df_engineered.shape[1] - df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4762e2",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Modeling\n",
    "\n",
    "Split into training/testing sets and scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252886e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "target = 'first_week'\n",
    "drop_columns = [target, 'movie_title', 'genre', 'director']\n",
    "\n",
    "# Separate features and target\n",
    "y = df_engineered[target]\n",
    "X = df_engineered.drop(columns=[col for col in drop_columns if col in df_engineered.columns])\n",
    "\n",
    "# Select only numeric columns\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Handle any remaining NaN or infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(\"Data preparation:\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns ({len(X.columns)}):\")\n",
    "print(list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56fce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train-Test Split:\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(f\"Train ratio: {len(X_train)/len(X)*100:.1f}%\")\n",
    "print(f\"Test ratio: {len(X_test)/len(X)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8280436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ“ Feature scaling complete using RobustScaler\")\n",
    "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23dc374",
   "metadata": {},
   "source": [
    "## 5. Build Base Models\n",
    "\n",
    "Train multiple base models to establish baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models\n",
    "base_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=10.0, random_state=42),\n",
    "    'Lasso': Lasso(alpha=1.0, random_state=42),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1),\n",
    "    'Extra Trees': ExtraTreesRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, n_jobs=-1),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, verbose=-1, n_jobs=-1),\n",
    "    'CatBoost': CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, random_state=42, verbose=0),\n",
    "    'SVR': SVR(kernel='rbf', C=100, gamma='auto'),\n",
    "    'KNN': KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
    "}\n",
    "\n",
    "print(f\"âœ“ {len(base_models)} base models defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate base models\n",
    "results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING BASE MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, \n",
    "                               cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Train MAE': train_mae,\n",
    "        'Test MAE': test_mae,\n",
    "        'Train RÂ²': train_r2,\n",
    "        'Test RÂ²': test_r2,\n",
    "        'CV RMSE': cv_rmse,\n",
    "        'Training Time (s)': training_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test RMSE: ${test_rmse:,.2f}\")\n",
    "    print(f\"  Test RÂ²: {test_r2:.4f}\")\n",
    "    print(f\"  Training time: {training_time:.2f}s\")\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Test RMSE')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASE MODELS TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a53a504",
   "metadata": {},
   "source": [
    "### Visualize Base Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Sort by Test RMSE\n",
    "results_sorted = results_df.sort_values('Test RMSE')\n",
    "\n",
    "# Plot 1: RMSE Comparison\n",
    "axes[0].barh(results_sorted['Model'], results_sorted['Test RMSE'], color='steelblue', alpha=0.8)\n",
    "axes[0].set_xlabel('Test RMSE ($)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Model Performance - RMSE (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: RÂ² Comparison\n",
    "results_r2_sorted = results_df.sort_values('Test RÂ²', ascending=True)\n",
    "axes[1].barh(results_r2_sorted['Model'], results_r2_sorted['Test RÂ²'], color='coral', alpha=0.8)\n",
    "axes[1].set_xlabel('Test RÂ² Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Model Performance - RÂ² (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Models by Test RMSE:\")\n",
    "print(results_df[['Model', 'Test RMSE', 'Test RÂ²']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03eeae5",
   "metadata": {},
   "source": [
    "## 6. Build Stacking Ensemble\n",
    "\n",
    "Stack multiple base learners with a meta-learner for improved predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb74fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base estimators for stacking\n",
    "base_estimators = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=150, max_depth=15, random_state=42, n_jobs=-1)),\n",
    "    ('xgb', xgb.XGBRegressor(n_estimators=150, learning_rate=0.05, max_depth=6, random_state=42, n_jobs=-1)),\n",
    "    ('lgb', lgb.LGBMRegressor(n_estimators=150, learning_rate=0.05, max_depth=6, random_state=42, verbose=-1, n_jobs=-1)),\n",
    "    ('catboost', CatBoostRegressor(iterations=150, learning_rate=0.05, depth=6, random_state=42, verbose=0)),\n",
    "    ('et', ExtraTreesRegressor(n_estimators=150, max_depth=15, random_state=42, n_jobs=-1))\n",
    "]\n",
    "\n",
    "# Meta-learner\n",
    "meta_learner = Ridge(alpha=10.0)\n",
    "\n",
    "# Create stacking regressor\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING STACKING ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBase learners: {len(base_estimators)}\")\n",
    "print(f\"Meta-learner: Ridge Regression\")\n",
    "print(f\"Cross-validation folds: 5\")\n",
    "print(\"\\nTraining...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train stacking model\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_stack = stacking_model.predict(X_train_scaled)\n",
    "y_test_pred_stack = stacking_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "stack_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_stack))\n",
    "stack_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_stack))\n",
    "stack_train_r2 = r2_score(y_train, y_train_pred_stack)\n",
    "stack_test_r2 = r2_score(y_test, y_test_pred_stack)\n",
    "stack_test_mae = mean_absolute_error(y_test, y_test_pred_stack)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ Stacking Ensemble trained in {training_time:.2f}s\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Train RMSE: ${stack_train_rmse:,.2f}\")\n",
    "print(f\"  Test RMSE:  ${stack_test_rmse:,.2f}\")\n",
    "print(f\"  Train RÂ²:   {stack_train_r2:.4f}\")\n",
    "print(f\"  Test RÂ²:    {stack_test_r2:.4f}\")\n",
    "print(f\"  Test MAE:   ${stack_test_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bbd85b",
   "metadata": {},
   "source": [
    "## 7. Build Voting Ensemble\n",
    "\n",
    "Combine multiple models using weighted voting for robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52208a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define voting ensemble with weighted models\n",
    "voting_estimators = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=150, max_depth=15, random_state=42, n_jobs=-1)),\n",
    "    ('xgb', xgb.XGBRegressor(n_estimators=150, learning_rate=0.05, max_depth=6, random_state=42, n_jobs=-1)),\n",
    "    ('lgb', lgb.LGBMRegressor(n_estimators=150, learning_rate=0.05, max_depth=6, random_state=42, verbose=-1, n_jobs=-1)),\n",
    "    ('catboost', CatBoostRegressor(iterations=150, learning_rate=0.05, depth=6, random_state=42, verbose=0)),\n",
    "]\n",
    "\n",
    "# Create voting regressor with weights (more weight to boosting models)\n",
    "voting_model = VotingRegressor(\n",
    "    estimators=voting_estimators,\n",
    "    weights=[1, 2, 2, 2],  # Give more weight to boosting algorithms\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING VOTING ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nEstimators: {len(voting_estimators)}\")\n",
    "print(f\"Weights: [1, 2, 2, 2] (RF, XGBoost, LightGBM, CatBoost)\")\n",
    "print(\"\\nTraining...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train voting model\n",
    "voting_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_vote = voting_model.predict(X_train_scaled)\n",
    "y_test_pred_vote = voting_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "vote_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_vote))\n",
    "vote_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_vote))\n",
    "vote_train_r2 = r2_score(y_train, y_train_pred_vote)\n",
    "vote_test_r2 = r2_score(y_test, y_test_pred_vote)\n",
    "vote_test_mae = mean_absolute_error(y_test, y_test_pred_vote)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ Voting Ensemble trained in {training_time:.2f}s\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Train RMSE: ${vote_train_rmse:,.2f}\")\n",
    "print(f\"  Test RMSE:  ${vote_test_rmse:,.2f}\")\n",
    "print(f\"  Train RÂ²:   {vote_train_r2:.4f}\")\n",
    "print(f\"  Test RÂ²:    {vote_test_r2:.4f}\")\n",
    "print(f\"  Test MAE:   ${vote_test_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ce0b58",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Comparison\n",
    "\n",
    "Compare all models including base models and ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86621a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ensemble results to the results dataframe\n",
    "ensemble_results = [\n",
    "    {\n",
    "        'Model': 'Stacking Ensemble',\n",
    "        'Train RMSE': stack_train_rmse,\n",
    "        'Test RMSE': stack_test_rmse,\n",
    "        'Test MAE': stack_test_mae,\n",
    "        'Train RÂ²': stack_train_r2,\n",
    "        'Test RÂ²': stack_test_r2,\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Voting Ensemble',\n",
    "        'Train RMSE': vote_train_rmse,\n",
    "        'Test RMSE': vote_test_rmse,\n",
    "        'Test MAE': vote_test_mae,\n",
    "        'Train RÂ²': vote_train_r2,\n",
    "        'Test RÂ²': vote_test_r2,\n",
    "    }\n",
    "]\n",
    "\n",
    "# Combine all results\n",
    "all_results_df = pd.concat([results_df, pd.DataFrame(ensemble_results)], ignore_index=True)\n",
    "all_results_df = all_results_df.sort_values('Test RMSE')\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"COMPLETE MODEL COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "print(f\"\\n{'Model':<25} {'Test RMSE':<18} {'Test MAE':<18} {'Test RÂ²':<10}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for _, row in all_results_df.iterrows():\n",
    "    print(f\"{row['Model']:<25} ${row['Test RMSE']:>16,.2f} ${row['Test MAE']:>16,.2f} {row['Test RÂ²']:>9.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"BEST MODEL: {all_results_df.iloc[0]['Model']}\")\n",
    "print(f\"Test RMSE: ${all_results_df.iloc[0]['Test RMSE']:,.2f}\")\n",
    "print(f\"Test RÂ²: {all_results_df.iloc[0]['Test RÂ²']:.4f}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize complete model comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "# Sort by Test RMSE\n",
    "all_results_sorted = all_results_df.sort_values('Test RMSE', ascending=True)\n",
    "\n",
    "# Create bar chart\n",
    "fig.add_trace(go.Bar(\n",
    "    y=all_results_sorted['Model'],\n",
    "    x=all_results_sorted['Test RMSE'],\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=all_results_sorted['Test RMSE'],\n",
    "        colorscale='RdYlGn_r',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"RMSE ($)\")\n",
    "    ),\n",
    "    text=[f\"${v:,.0f}\" for v in all_results_sorted['Test RMSE']],\n",
    "    textposition='auto',\n",
    "    hovertemplate='<b>%{y}</b><br>RMSE: $%{x:,.0f}<br>RÂ²: %{customdata:.4f}<extra></extra>',\n",
    "    customdata=all_results_sorted['Test RÂ²']\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Complete Model Performance Comparison (Test RMSE)',\n",
    "    xaxis_title='Test RMSE ($)',\n",
    "    yaxis_title='Model',\n",
    "    height=max(500, len(all_results_sorted) * 30),\n",
    "    showlegend=False,\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# RÂ² comparison\n",
    "fig2 = go.Figure()\n",
    "\n",
    "all_results_r2_sorted = all_results_df.sort_values('Test RÂ²', ascending=True)\n",
    "\n",
    "fig2.add_trace(go.Bar(\n",
    "    y=all_results_r2_sorted['Model'],\n",
    "    x=all_results_r2_sorted['Test RÂ²'],\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=all_results_r2_sorted['Test RÂ²'],\n",
    "        colorscale='Viridis',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"RÂ² Score\")\n",
    "    ),\n",
    "    text=[f\"{v:.4f}\" for v in all_results_r2_sorted['Test RÂ²']],\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='Model Performance Comparison (Test RÂ² Score)',\n",
    "    xaxis_title='Test RÂ² Score',\n",
    "    yaxis_title='Model',\n",
    "    height=max(500, len(all_results_r2_sorted) * 30),\n",
    "    showlegend=False,\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a593cd",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning\n",
    "\n",
    "Fine-tune the best performing model for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for XGBoost (typically one of the best performers)\n",
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - XGBoost\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "print(\"\\nParameter grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "print(\"\\nPerforming GridSearchCV with 3-fold cross-validation...\")\n",
    "print(\"(This may take a few minutes...)\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"\\nâœ“ Grid Search Complete!\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Evaluate best model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_test_pred_tuned = best_xgb.predict(X_test_scaled)\n",
    "\n",
    "tuned_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_tuned))\n",
    "tuned_r2 = r2_score(y_test, y_test_pred_tuned)\n",
    "tuned_mae = mean_absolute_error(y_test, y_test_pred_tuned)\n",
    "\n",
    "print(f\"\\nTuned XGBoost Performance:\")\n",
    "print(f\"  Test RMSE: ${tuned_rmse:,.2f}\")\n",
    "print(f\"  Test RÂ²: {tuned_r2:.4f}\")\n",
    "print(f\"  Test MAE: ${tuned_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690c649",
   "metadata": {},
   "source": [
    "## 10. Final Model Selection and Prediction for Avatar: Ash & Fire\n",
    "\n",
    "Use the best model to predict first week income for Avatar: Ash & Fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fe2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for Avatar: Ash & Fire prediction\n",
    "avatar_data = {\n",
    "    'budget': 450000000,  # Estimated production budget\n",
    "    'runtime': 185,\n",
    "    'imdb_rating': 8.2,  # Expected rating based on franchise\n",
    "    'imdb_votes': 750000,\n",
    "    'metascore': 82,\n",
    "    'num_reviews': 2200,\n",
    "    'num_critic_reviews': 180,\n",
    "    'num_theaters': 4600,  # Wide release\n",
    "    'opening_weekend': 220000000,  # Estimated\n",
    "    'average_per_theater': 65000,\n",
    "    \n",
    "    # T-7 Trend Data\n",
    "    'twitter_mentions': 220000,\n",
    "    'twitter_sentiment': 0.88,\n",
    "    'google_trends_score': 98,\n",
    "    'youtube_trailer_views': 55000000,\n",
    "    'youtube_trailer_likes': 2200000,\n",
    "    'instagram_hashtag_count': 580000,\n",
    "    'facebook_page_likes': 5500000,\n",
    "    'reddit_mentions': 14000,\n",
    "    'search_volume_index': 97,\n",
    "    'ticket_presales': 22000000,\n",
    "    \n",
    "    # Temporal features\n",
    "    'release_month': 12,\n",
    "    'release_day_of_week': 4,  # Friday\n",
    "    'is_holiday_season': 1,\n",
    "    'is_summer': 0,\n",
    "    'competing_releases_same_week': 1,\n",
    "    \n",
    "    # Franchise features\n",
    "    'is_sequel': 1,\n",
    "    'franchise_previous_avg_gross': 722000000,\n",
    "    'years_since_last_release': 3,\n",
    "    \n",
    "    # Metadata\n",
    "    'movie_title': 'Avatar: Ash & Fire',\n",
    "    'genre': 'Action|Sci-Fi|Adventure',\n",
    "    'director': 'James Cameron'\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "avatar_df = pd.DataFrame([avatar_data])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AVATAR: ASH & FIRE - MOVIE DATA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nMovie: {avatar_df['movie_title'].values[0]}\")\n",
    "print(f\"Director: {avatar_df['director'].values[0]}\")\n",
    "print(f\"Genre: {avatar_df['genre'].values[0]}\")\n",
    "print(f\"Budget: ${avatar_df['budget'].values[0]:,.0f}\")\n",
    "print(f\"\\nT-7 Metrics:\")\n",
    "print(f\"  Twitter Mentions: {avatar_df['twitter_mentions'].values[0]:,}\")\n",
    "print(f\"  Twitter Sentiment: {avatar_df['twitter_sentiment'].values[0]:.2f}\")\n",
    "print(f\"  Google Trends: {avatar_df['google_trends_score'].values[0]:.0f}\")\n",
    "print(f\"  YouTube Views: {avatar_df['youtube_trailer_views'].values[0]:,}\")\n",
    "print(f\"  Ticket Pre-sales: ${avatar_df['ticket_presales'].values[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a23f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer features for Avatar prediction\n",
    "avatar_engineered = engineer_features(avatar_df)\n",
    "\n",
    "# Prepare features (same as training)\n",
    "avatar_X = avatar_engineered.drop(columns=[col for col in drop_columns if col in avatar_engineered.columns])\n",
    "avatar_X = avatar_X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Ensure all features match training features\n",
    "for col in X.columns:\n",
    "    if col not in avatar_X.columns:\n",
    "        avatar_X[col] = 0\n",
    "\n",
    "# Reorder columns to match training data\n",
    "avatar_X = avatar_X[X.columns]\n",
    "\n",
    "# Scale features\n",
    "avatar_X_scaled = scaler.transform(avatar_X)\n",
    "\n",
    "print(\"\\nâœ“ Features prepared for prediction\")\n",
    "print(f\"Feature vector shape: {avatar_X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with all ensemble models\n",
    "predictions = {}\n",
    "\n",
    "# Stacking Ensemble\n",
    "pred_stack = stacking_model.predict(avatar_X_scaled)[0]\n",
    "predictions['Stacking Ensemble'] = pred_stack\n",
    "\n",
    "# Voting Ensemble\n",
    "pred_vote = voting_model.predict(avatar_X_scaled)[0]\n",
    "predictions['Voting Ensemble'] = pred_vote\n",
    "\n",
    "# Tuned XGBoost\n",
    "pred_xgb = best_xgb.predict(avatar_X_scaled)[0]\n",
    "predictions['Tuned XGBoost'] = pred_xgb\n",
    "\n",
    "# Top base models\n",
    "top_models = all_results_df.head(3)\n",
    "for _, row in top_models.iterrows():\n",
    "    model_name = row['Model']\n",
    "    if model_name in base_models:\n",
    "        pred = base_models[model_name].predict(avatar_X_scaled)[0]\n",
    "        predictions[model_name] = pred\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FIRST WEEK BOX OFFICE PREDICTIONS - AVATAR: ASH & FIRE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Model':<30} {'Prediction':<20}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for model_name, pred_value in predictions.items():\n",
    "    print(f\"{model_name:<30} ${pred_value:>18,.2f}\")\n",
    "\n",
    "# Calculate ensemble average\n",
    "avg_prediction = np.mean(list(predictions.values()))\n",
    "median_prediction = np.median(list(predictions.values()))\n",
    "std_prediction = np.std(list(predictions.values()))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENSEMBLE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mean Prediction:     ${avg_prediction:>15,.2f}\")\n",
    "print(f\"Median Prediction:   ${median_prediction:>15,.2f}\")\n",
    "print(f\"Std Deviation:       ${std_prediction:>15,.2f}\")\n",
    "print(f\"Min Prediction:      ${min(predictions.values()):>15,.2f}\")\n",
    "print(f\"Max Prediction:      ${max(predictions.values()):>15,.2f}\")\n",
    "print(f\"\\nConfidence Range:    ${avg_prediction * 0.85:>15,.2f} - ${avg_prediction * 1.15:,.2f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea53311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig = go.Figure()\n",
    "\n",
    "pred_df = pd.DataFrame(list(predictions.items()), columns=['Model', 'Prediction'])\n",
    "pred_df = pred_df.sort_values('Prediction', ascending=True)\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=pred_df['Model'],\n",
    "    x=pred_df['Prediction'],\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=pred_df['Prediction'],\n",
    "        colorscale='Plasma',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Prediction ($)\")\n",
    "    ),\n",
    "    text=[f\"${v:,.0f}\" for v in pred_df['Prediction']],\n",
    "    textposition='auto',\n",
    "    hovertemplate='<b>%{y}</b><br>Prediction: $%{x:,.0f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add mean line\n",
    "fig.add_vline(x=avg_prediction, line_dash=\"dash\", line_color=\"red\", line_width=2,\n",
    "             annotation_text=f\"Mean: ${avg_prediction:,.0f}\", annotation_position=\"top\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Avatar: Ash & Fire - First Week Income Predictions by Model',\n",
    "    xaxis_title='Predicted First Week Income ($)',\n",
    "    yaxis_title='Model',\n",
    "    height=500,\n",
    "    showlegend=False,\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77fb6a7",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis\n",
    "\n",
    "Analyze which features contribute most to the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from best XGBoost model\n",
    "feature_importance = best_xgb.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TOP 20 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Rank':<6} {'Feature':<40} {'Importance':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for idx, (_, row) in enumerate(importance_df.head(20).iterrows(), 1):\n",
    "    print(f\"{idx:<6} {row['Feature']:<40} {row['Importance']:<15.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d7a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "top_n = 20\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=top_features['Feature'],\n",
    "    x=top_features['Importance'],\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=top_features['Importance'],\n",
    "        colorscale='Teal',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Importance\")\n",
    "    ),\n",
    "    hovertemplate='<b>%{y}</b><br>Importance: %{x:.6f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Top {top_n} Most Important Features (XGBoost)',\n",
    "    xaxis_title='Feature Importance',\n",
    "    yaxis_title='Feature',\n",
    "    height=max(500, top_n * 25),\n",
    "    showlegend=False,\n",
    "    font=dict(size=11)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Categorize features by source\n",
    "print(\"\\nFeature Importance by Data Source:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "sources = {\n",
    "    'IMDB': ['imdb_rating', 'imdb_votes', 'metascore', 'num_reviews', 'num_critic_reviews', 'quality_score'],\n",
    "    'Box Office': ['budget', 'num_theaters', 'opening_weekend', 'average_per_theater', 'budget_per_theater', 'theater_efficiency'],\n",
    "    'T-7 Trends': ['twitter_mentions', 'twitter_sentiment', 'google_trends_score', 'youtube_trailer_views', \n",
    "                   'youtube_trailer_likes', 'ticket_presales', 'instagram_hashtag_count', 'facebook_page_likes',\n",
    "                   'social_engagement_score', 'hype_index'],\n",
    "    'Franchise': ['franchise_previous_avg_gross', 'franchise_momentum', 'years_since_last_release', 'is_sequel'],\n",
    "    'Temporal': ['release_month', 'release_day_of_week', 'is_holiday_season', 'is_summer']\n",
    "}\n",
    "\n",
    "for source, features in sources.items():\n",
    "    source_importance = importance_df[importance_df['Feature'].isin(features)]['Importance'].sum()\n",
    "    print(f\"{source:<20} {source_importance:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1f3ea",
   "metadata": {},
   "source": [
    "## 12. Final Summary and Recommendations\n",
    "\n",
    "Comprehensive project summary and key insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b9c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=\"*80)\n",
    "print(\"AVATAR: ASH & FIRE - BOX OFFICE PREDICTION PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š PROJECT OVERVIEW\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Dataset Size: {len(df)} historical movies\")\n",
    "print(f\"Features Created: {len(X.columns)} features\")\n",
    "print(f\"Models Trained: {len(base_models) + 2} (13 base + 2 ensembles)\")\n",
    "print(f\"Training/Test Split: 80/20\")\n",
    "\n",
    "print(\"\\nðŸ† BEST PERFORMING MODEL\")\n",
    "print(\"-\"*80)\n",
    "best_model_name = all_results_df.iloc[0]['Model']\n",
    "best_rmse = all_results_df.iloc[0]['Test RMSE']\n",
    "best_r2 = all_results_df.iloc[0]['Test RÂ²']\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"Test RMSE: ${best_rmse:,.2f}\")\n",
    "print(f\"Test RÂ²: {best_r2:.4f}\")\n",
    "print(f\"Accuracy: {best_r2 * 100:.2f}% of variance explained\")\n",
    "\n",
    "print(\"\\nðŸŽ¬ AVATAR: ASH & FIRE PREDICTIONS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Mean Prediction: ${avg_prediction:,.2f}\")\n",
    "print(f\"Median Prediction: ${median_prediction:,.2f}\")\n",
    "print(f\"Confidence Range: ${avg_prediction * 0.85:,.2f} - ${avg_prediction * 1.15:,.2f}\")\n",
    "print(f\"\\nBest Single Model: ${predictions['Tuned XGBoost']:,.2f}\")\n",
    "print(f\"Stacking Ensemble: ${predictions['Stacking Ensemble']:,.2f}\")\n",
    "print(f\"Voting Ensemble: ${predictions['Voting Ensemble']:,.2f}\")\n",
    "\n",
    "print(\"\\nðŸ” KEY INSIGHTS\")\n",
    "print(\"-\"*80)\n",
    "top_3_features = importance_df.head(3)\n",
    "print(\"Top 3 Most Important Features:\")\n",
    "for idx, (_, row) in enumerate(top_3_features.iterrows(), 1):\n",
    "    print(f\"  {idx}. {row['Feature']}: {row['Importance']:.6f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ RECOMMENDATIONS\")\n",
    "print(\"-\"*80)\n",
    "print(\"1. Use ensemble methods (Stacking/Voting) for most accurate predictions\")\n",
    "print(\"2. T-7 trend data (social media engagement) is highly predictive\")\n",
    "print(\"3. Budget and theater count remain strong indicators\")\n",
    "print(\"4. Pre-sales data provides early signal of box office performance\")\n",
    "print(\"5. Franchise momentum and brand value significantly impact predictions\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ MODEL COMPARISON HIGHLIGHTS\")\n",
    "print(\"-\"*80)\n",
    "print(\"Top 5 Models by Test RMSE:\")\n",
    "for idx, (_, row) in enumerate(all_results_df.head(5).iterrows(), 1):\n",
    "    print(f\"  {idx}. {row['Model']:<30} RMSE: ${row['Test RMSE']:>12,.2f}  RÂ²: {row['Test RÂ²']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… PROJECT COMPLETE - PREDICTIONS GENERATED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create summary dataframe\n",
    "summary = {\n",
    "    'Metric': ['Dataset Size', 'Features', 'Models Trained', 'Best Model', \n",
    "               'Best Test RMSE', 'Best Test RÂ²', 'Avatar Prediction (Mean)', \n",
    "               'Avatar Prediction (Best Model)'],\n",
    "    'Value': [len(df), len(X.columns), len(base_models) + 2, best_model_name,\n",
    "              f\"${best_rmse:,.2f}\", f\"{best_r2:.4f}\", f\"${avg_prediction:,.2f}\",\n",
    "              f\"${predictions['Tuned XGBoost']:,.2f}\"]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n\")\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
